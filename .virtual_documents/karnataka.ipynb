import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset



from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error
import torch.optim.lr_scheduler as lr_scheduler
import torch.nn.functional as F
import joblib
import numpy as np



data=pd.read_csv("karnataka_power_consumption2.csv")


data.head(2)


df=data
df


df.isna().sum()


df.info()


cols_encoder=['City','State','Month','Weather Condition','Power Supply Status']


encoders_dict = {}  # For mapping original to encoded values
label_encoders = {}  # For storing LabelEncoder instances

# Encode each column
for col in cols_encoder:
    # Convert values to lowercase for consistency
    df[col] = df[col].str.lower()
    
    # Initialize LabelEncoder
    le = LabelEncoder()
    
    # Fit and transform the column
    df[col] = le.fit_transform(df[col])
    
    # Store mappings in encoders_dict
    encoders_dict[col] = dict(zip(le.classes_, range(len(le.classes_))))
    
    # Store LabelEncoder instance
    label_encoders[col] = le




encoders_dict



df.head(5)


df.tail(5)


df.columns


input_columns = ['City', 'State', 'Month', 'Weather Condition', 'Temperature (Â°C)',
       'Humidity (%)', 'Rainfall Chances (%)', 'Wind Speed (km/h)',
       'Fan Power Consumed (kWh)', 'Light Power Consumed (kWh)',
       'Mixer Power Consumed (kWh)', 'Washing Machine Power Consumed (kWh)',
       'Phone Charging Power Consumed (kWh)', 'UPS Power Consumed (kWh)',
       'Grinder Power Consumed (kWh)', 'AC Power Consumed (kWh)',
       'Heater Power Consumed (kWh)', 'Fridge Power Consumed (kWh)',
       'TV Power Consumed (kWh)']
target_columns = ['Total Power Consumed (kWh)',
       'Required Power Supply (kW)', 'Current Power Supply (kW)',
       'Power Supply Status']



#avg_features = df.groupby(["City",'State', 'Month']).mean().reset_index()


#avg_features


#avg_features.to_csv("processed_features.csv", index=False)








X=df[input_columns].values
y=df[target_columns].values
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)


scaler_X = StandardScaler()
scaler_y = StandardScaler()



df


print(X.shape)
print(y.shape)



X_train = scaler_X.fit_transform(X_train)
X_val = scaler_X.transform(X_val)
y_train = scaler_y.fit_transform(y_train)
y_val = scaler_y.transform(y_val)


joblib.dump(scaler_X, 'scaler_X.pkl')
joblib.dump(scaler_y, 'scaler_y.pkl')


X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32)


train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)


import torch.nn.functional as F


class PowerConsumptionModel(nn.Module):
    def __init__(self, input_size, output_size):
        super(PowerConsumptionModel, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 256),
            nn.BatchNorm1d(256), 
            nn.ELU(),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ELU(),
            nn.Linear(128, 64),
            nn.LeakyReLU(negative_slope=0.02),
            nn.Linear(64,output_size ),
            nn.Softplus()

        )

    def forward(self, x):
        return self.network(x)


input_size = X_train.shape[1]
output_size = y_train.shape[1]
model = PowerConsumptionModel(input_size, output_size)
criterion = nn.SmoothL1Loss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)


scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)

epochs = 100
patience = 10
best_val_loss = float('inf')
patience_counter = 0

for epoch in range(epochs):
    model.train()
    train_loss = 0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        predictions = model(X_batch)
        loss = criterion(predictions, y_batch)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    model.eval()
    val_loss = 0
    val_mae = 0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)
            val_loss += loss.item()

            mae = torch.mean(torch.abs(predictions - y_batch))
            val_mae += mae.item()

    train_loss /= len(train_loader)
    val_loss /= len(val_loader)
    val_mae /= len(val_loader)

    print(f"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

    # Step the scheduler based on the validation loss
    scheduler.step(val_loss)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        torch.save(model.state_dict(), "best_model.pth") 
    else:
        patience_counter += 1

    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch+1}")
        break

model.load_state_dict(torch.load("best_model.pth"))
print("Best model loaded.")




# Define the model architecture (same as training)
model = PowerConsumptionModel(input_size, output_size)  # Replace with your actual model class
model.load_state_dict(torch.load("best_model.pth"))
model.eval()  # Set to evaluation mode





# Define the model architecture (same as training)
model = PowerConsumptionModel(input_size, output_size)  # Replace with your actual model class
model.load_state_dict(torch.load("best_model.pth"))
model.eval()  # Set to evaluation mode



import torch
import numpy as np
from sklearn.preprocessing import StandardScaler

num_districts = 31
num_features = 19
num_outputs = 4

# Assuming scaler_X and scaler_y are already fitted
# Assuming model is already trained

# Generate random input data for 31 districts
random_data = np.random.uniform(low=[5, 5, 0, 0, 20, 20, 30, 2, 0.5, 0.05, 1.0, 5.0, 0.01, 2.5, 0.5, 5.0, 5.0, 3.0, 2.0],
                                 high=[10, 10, 1, 1, 35, 35, 50, 10, 0.7, 0.2, 1.5, 8.0, 0.05, 4.0, 1.2, 9.0, 9.0, 5.0, 4.0],
                                 size=(num_districts, num_features))

# Transform input using scaler
tensor_input = scaler_X.transform(random_data)
tensor_input = torch.tensor(tensor_input, dtype=torch.float32)

with torch.no_grad():
    output = model(tensor_input)

# Inverse transform the predicted output
predicted_output = scaler_y.inverse_transform(output.numpy())

# Adjust second and third column relationships randomly
for i in range(num_districts):
    condition = np.random.choice(["greater", "less", "equal"])
    if condition == "greater":
        if predicted_output[i, 1] <= predicted_output[i, 2]:
            predicted_output[i, 1] += 0.1
            predicted_output[i, 2] -= 0.1
    elif condition == "less":
        if predicted_output[i, 1] >= predicted_output[i, 2]:
            predicted_output[i, 1] -= 0.1
            predicted_output[i, 2] += 0.1
    else:  # "equal"
        predicted_output[i, 1] = (predicted_output[i, 1] + predicted_output[i, 2]) / 2
        predicted_output[i, 2] = predicted_output[i, 1]
    
    print(f"District {i+1}: Predicted Outputs:", predicted_output[i])










